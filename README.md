# ğŸ§  Neural Network from Scratch with NumPy
Build, Train, and Understand Every Layer â€” No Frameworks Needed!

## ğŸ” Overview
This project implements a complete Multi-Layer Neural Network using only NumPy. It includes forward propagation, backpropagation, cross-entropy loss, L2 regularization, and gradient checking â€” all applied to the classic Iris dataset ğŸŒ¸

## ğŸ§ª Features
ğŸ§® Built from scratch using NumPy

ğŸ” Two hidden layers with ReLU activation

ğŸ¯ Softmax output layer for multi-class classification

âŒ No ML/DL libraries used (no TensorFlow, PyTorch)

ğŸ§  Cross-Entropy Loss + L2 Regularization

âœ… Gradient checking for debugging backprop

ğŸ’¯ Achieves 100% accuracy on test set


### ğŸ“Š Dataset
Iris Dataset from sklearn.datasets:

150 samples

4 input features

3 classes (Setosa, Versicolor, Virginica)

## ğŸ’¡ What I Learned
Inner workings of neural nets: weights, gradients, activations

Debugging with gradient checks

Applying L2 regularization to combat overfitting

Achieving perfect accuracy with hand-crafted backprop!

## ğŸ”— Connect With Me
ğŸ’¼ LinkedIn

ğŸ§  Medium Article (coming soon)

ğŸ’Œ DM me if you want to collab or have feedback!
