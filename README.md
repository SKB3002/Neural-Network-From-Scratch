# 🧠 Neural Network from Scratch with NumPy
Build, Train, and Understand Every Layer — No Frameworks Needed!

## 🔍 Overview
This project implements a complete Multi-Layer Neural Network using only NumPy. It includes forward propagation, backpropagation, cross-entropy loss, L2 regularization, and gradient checking — all applied to the classic Iris dataset 🌸

## 🧪 Features
🧮 Built from scratch using NumPy

🔁 Two hidden layers with ReLU activation

🎯 Softmax output layer for multi-class classification

❌ No ML/DL libraries used (no TensorFlow, PyTorch)

🧠 Cross-Entropy Loss + L2 Regularization

✅ Gradient checking for debugging backprop

💯 Achieves 100% accuracy on test set


### 📊 Dataset
Iris Dataset from sklearn.datasets:

150 samples

4 input features

3 classes (Setosa, Versicolor, Virginica)

## 💡 What I Learned
Inner workings of neural nets: weights, gradients, activations

Debugging with gradient checks

Applying L2 regularization to combat overfitting

Achieving perfect accuracy with hand-crafted backprop!

## 🔗 Connect With Me
💼 LinkedIn

🧠 Medium Article (coming soon)

💌 DM me if you want to collab or have feedback!
