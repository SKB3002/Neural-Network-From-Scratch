{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setting Up Activation Function ","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef layer(inputs, weights, bias, activation):\n    z = np.dot(inputs, weights) + bias\n    if activation == \"sigmoid\":\n        return 1 / (1 + np.exp(-z))\n    elif activation == \"relu\":\n        return np.maximum(0, z)\n    elif activation == 'softmax':\n        exp_x = np.exp(z - np.max(z, axis=1, keepdims=True))  # for numerical stability\n        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n    else:\n        return z  # Linear (no activation)\n\n# Example\ninputs = np.array([2, 3])\nweights = np.array([0.5, 1.2])\nbias = -1\n\noutput = layer(inputs, weights, bias, \"relu\")\nprint(\"Neuron Output:\", output)  # Output: 3.6","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:04:09.550074Z","iopub.execute_input":"2025-04-12T14:04:09.550406Z","iopub.status.idle":"2025-04-12T14:04:09.569606Z","shell.execute_reply.started":"2025-04-12T14:04:09.550361Z","shell.execute_reply":"2025-04-12T14:04:09.568798Z"}},"outputs":[{"name":"stdout","text":"Neuron Output: 3.5999999999999996\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Building Single Neuron","metadata":{}},{"cell_type":"code","source":"# Example: Negative weighted sum\ninputs = np.array([-1, -2])\nweights = np.array([0.5, 1.2])\nbias = 0.1\noutput2 = layer(inputs, weights, bias, \"sigmoid\")  # Output: 0.0 (ReLU kills negative z)\n\n# Exmple: Sigmoid with z=0\noutput1 = layer(np.array([[2.0, -1.0]]), weights, bias=0, activation=\"relu\")  # Output: 0.5\nprint(\"Neuron Output:\", output2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:04:11.845650Z","iopub.execute_input":"2025-04-12T14:04:11.845948Z","iopub.status.idle":"2025-04-12T14:04:11.852806Z","shell.execute_reply.started":"2025-04-12T14:04:11.845926Z","shell.execute_reply":"2025-04-12T14:04:11.851631Z"}},"outputs":[{"name":"stdout","text":"Neuron Output: 0.057324175898868755\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Building A Layer of Neuron ","metadata":{}},{"cell_type":"code","source":"X = np.array([[2.0, -1.0, 3.0,-5]])\n\nweights = np.random.rand(4, 8) # shape (2, 3)\nbias1 = np.random.rand(1,8) \noutput3 = layer(X, weights = weights, bias=bias1, activation=\"sigmoid\")  \nprint(\"Neuron Output:\", output3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:04:13.283483Z","iopub.execute_input":"2025-04-12T14:04:13.283798Z","iopub.status.idle":"2025-04-12T14:04:13.291179Z","shell.execute_reply.started":"2025-04-12T14:04:13.283774Z","shell.execute_reply":"2025-04-12T14:04:13.290457Z"}},"outputs":[{"name":"stdout","text":"Neuron Output: [[0.02662472 0.82093516 0.33889673 0.7970821  0.94727045 0.27951157\n  0.74238858 0.04885937]]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"X = np.array([[2.0, -1.0, 3.0,2.2]])\ninput_size = X.shape[1]     # 3\nnum_neurons = 4\n\nw1 = np.random.randn(input_size, num_neurons)  # shape: (3, 5)\nb1 = np.random.randn(1, num_neurons)              # shape: (1, 5)\nout5 = layer(X, weights = w1, bias = b1 , activation=\"relu\")\n\n\ninp_size_2 = out5.shape[1]\nw2 = np.random.randn(inp_size_2, 2)\nb2 = np.random.randn(1, 2)              # shape: (1, 5)\nout6 = layer(out5, w2, b2, activation=\"sigmoid\") \nprint(out6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:04:24.872331Z","iopub.execute_input":"2025-04-12T14:04:24.872677Z","iopub.status.idle":"2025-04-12T14:04:24.880618Z","shell.execute_reply.started":"2025-04-12T14:04:24.872654Z","shell.execute_reply":"2025-04-12T14:04:24.879610Z"}},"outputs":[{"name":"stdout","text":"[[0.16570231 0.99531169]]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"input_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:04:27.442602Z","iopub.execute_input":"2025-04-12T14:04:27.442874Z","iopub.status.idle":"2025-04-12T14:04:27.449730Z","shell.execute_reply.started":"2025-04-12T14:04:27.442853Z","shell.execute_reply":"2025-04-12T14:04:27.448552Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"4"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# Building Multi Layer Network","metadata":{}},{"cell_type":"code","source":"X = np.array([[2.0, -1.0, 3.0,2.2]])\ninput_size = X.shape[1]     # 3\nnum_neurons = 5\n\n# Layer 1\nw1 = np.random.randn(input_size, num_neurons)     \nb1 = np.random.randn(1, num_neurons)              \nout1 = layer(X, weights=w1, bias=b1, activation=\"relu\")\n\n#LAyer 2\ninput_size_2 = out1.shape[1]  \nw2 = np.random.randn(input_size, num_neurons)     \nb2 = np.random.randn(1, num_neurons)             \nout2 = layer(X, weights=w2, bias=b2, activation=\"relu\")\n\n# Layer 3\ninput_size_3 = out1.shape[1]                      \nw3 = np.random.randn(input_size_3, 3)   \nb3 = np.random.randn(1, 3)              \nout3 = layer(out1, weights=w3, bias=b3, activation=\"softmax\")\n\nprint(\"Final Output :\", out3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:04:29.546837Z","iopub.execute_input":"2025-04-12T14:04:29.547117Z","iopub.status.idle":"2025-04-12T14:04:29.555952Z","shell.execute_reply.started":"2025-04-12T14:04:29.547097Z","shell.execute_reply":"2025-04-12T14:04:29.555036Z"}},"outputs":[{"name":"stdout","text":"Final Output : [[0.16758159 0.00117735 0.83124106]]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Setting Up Iris Dataset","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Load data\niris = load_iris()\nX = iris.data  \ny = iris.target.reshape(-1, 1)\n\n# Normalize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# One-hot encode labels\nencoder = OneHotEncoder(sparse_output=False)\ny_encoded = encoder.fit_transform(y)  \n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)\n\nprint(\"Train shape:\", X_train.shape)\nprint(\"Test shape:\", X_test.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:04:31.421069Z","iopub.execute_input":"2025-04-12T14:04:31.421375Z","iopub.status.idle":"2025-04-12T14:04:32.229007Z","shell.execute_reply.started":"2025-04-12T14:04:31.421352Z","shell.execute_reply":"2025-04-12T14:04:32.227688Z"}},"outputs":[{"name":"stdout","text":"Train shape: (120, 4)\nTest shape: (30, 4)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Passing Our Data Through our 3 Layer Model","metadata":{}},{"cell_type":"code","source":"in_s = int(X_train.shape[1])\nw1 = np.random.randn(in_s, 10) \nb1 = np.zeros((1, 10))\nout1 = layer(X_train,w1,b1,\"relu\")\n\nin_s2 = out1.shape[1]\nw2 = np.random.randn(in_s2, 8)   \nb2 = np.zeros((1, 8))           \nout2 = layer(out1, weights=w2, bias=b2, activation=\"relu\")\n\nin_s3 = out2.shape[1]\nw3 = np.random.randn(in_s3, 3)   \nb3 = np.zeros((1, 3))           \ny_pred = layer(out2, weights=w3, bias=b3, activation=\"softmax\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:04:42.465237Z","iopub.execute_input":"2025-04-12T14:04:42.466103Z","iopub.status.idle":"2025-04-12T14:04:42.486559Z","shell.execute_reply.started":"2025-04-12T14:04:42.466072Z","shell.execute_reply":"2025-04-12T14:04:42.485587Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Loss Function","metadata":{}},{"cell_type":"code","source":"def cross_entropy_loss(predictions, y_true):\n    # Add epsilon for numerical stability\n    eps = 1e-15\n    predictions = np.clip(predictions, eps, 1 - eps)\n    loss = -np.mean(np.sum(y_true * np.log(predictions + 1e-9), axis=1))\n    return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:05:22.110984Z","iopub.execute_input":"2025-04-12T14:05:22.111308Z","iopub.status.idle":"2025-04-12T14:05:22.116943Z","shell.execute_reply.started":"2025-04-12T14:05:22.111284Z","shell.execute_reply":"2025-04-12T14:05:22.115708Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"cross_entropy_loss(y_train,y_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:05:23.230232Z","iopub.execute_input":"2025-04-12T14:05:23.230754Z","iopub.status.idle":"2025-04-12T14:05:23.241115Z","shell.execute_reply.started":"2025-04-12T14:05:23.230719Z","shell.execute_reply":"2025-04-12T14:05:23.240170Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"13.117765631472084"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"y_pred.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:05:23.912889Z","iopub.execute_input":"2025-04-12T14:05:23.913185Z","iopub.status.idle":"2025-04-12T14:05:23.918918Z","shell.execute_reply.started":"2025-04-12T14:05:23.913162Z","shell.execute_reply":"2025-04-12T14:05:23.917908Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(120, 3)"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"# Derivative Function","metadata":{}},{"cell_type":"code","source":"def relu_derivative(z):\n    return np.where(z > 0, 1, 0)  # Return 1 if z > 0, else 0\n\n\ndef softmax(z):\n    exp_x = np.exp(z - np.max(z, axis=1, keepdims=True))  # for numerical stability\n    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n\ndef softmax_derivative(z, y_true):\n    softmax_output = softmax(z)\n\n    return softmax_output - y_true\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:05:25.733439Z","iopub.execute_input":"2025-04-12T14:05:25.733738Z","iopub.status.idle":"2025-04-12T14:05:25.739781Z","shell.execute_reply.started":"2025-04-12T14:05:25.733716Z","shell.execute_reply":"2025-04-12T14:05:25.738582Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Full Model with Backpropagation","metadata":{}},{"cell_type":"code","source":"input_size = X_train.shape[1]  # 4\nhidden_size = 10\noutput_size = 3  # 3 classes in Iris\n\n# Weights & Biases\nw1 = np.random.randn(input_size, hidden_size)\nb1 = np.zeros((1, hidden_size))\nw2 = np.random.randn(hidden_size, hidden_size)\nb2 = np.zeros((1, hidden_size))\nw3 = np.random.randn(hidden_size, output_size)\nb3 = np.zeros((1, output_size))\n\n# ---- Training ----\nlr = 0.01\nepochs = 500\n\nfor epoch in range(epochs):\n    # Forward Pass\n    z1 = layer(X_train,w1,b1, activation = 'None')\n    out1 = layer(X_train,w1,b1,activation =\"relu\")\n\n    z2 = layer(out1, w2,b2,activation = 'None')\n    out2 = layer(out1, w2, b2, activation = \"relu\")\n\n    z3 = layer(out2, w3,b3,activation = 'None')\n    out3 = layer(out2, w3,b3, activation = 'softmax')\n    \n\n    # Loss\n    loss = cross_entropy_loss(out3, y_train)\n\n    # Backward Pass\n    dz3 = softmax_derivative(z3, y_train)  # (batch, 3)\n    dw3 = np.dot(out2.T, dz3)\n    db3 = np.sum(dz3, axis=0, keepdims=True)\n\n    dz2 = np.dot(dz3, w3.T) * relu_derivative(z2)\n    dw2 = np.dot(out1.T, dz2)\n    db2 = np.sum(dz2, axis=0, keepdims=True)\n\n    dz1 = np.dot(dz2, w2.T) * relu_derivative(z1)\n    dw1 = np.dot(X_train.T, dz1)\n    db1 = np.sum(dz1, axis=0, keepdims=True)\n\n    # Update weights\n    w3 -= lr * dw3\n    b3 -= lr * db3\n    w2 -= lr * dw2\n    b2 -= lr * db2\n    w1 -= lr * dw1\n    b1 -= lr * db1\n    \n    if epoch == 10:\n        print(\"\\n--- Gradient Check on w1[0,0] ---\")\n        #gradient_check(X_train[:5], y_train[:5], w1, b1, w3, b3, dw1)\n        #gradient_check(X_train, y_train, w1, b1, w2, b2, w3, b3, dw1)\n        gradient_check(X_train, y_train, w1, b1, w2, b2, w3, b3, dw1,(0, 0))\n\n\n\n    if epoch % 50 == 0:\n        print(f\"Epoch {epoch} - Loss: {loss:.4f}\")\n        \n# ---- Accuracy on Test Set ----\ndef accuracy(X, y_true):\n    z1 = np.dot(X, w1) + b1\n    a1 = np.maximum(0, z1)\n\n    z2 =  np.dot(a1, w2) + b2\n    a2 = np.maximum(0, z2)\n\n    z3 = np.dot(a2, w3) + b3\n    probs = softmax(z3)\n\n    predictions = np.argmax(probs, axis=1)\n    labels = np.argmax(y_true, axis=1)\n\n    return np.mean(predictions == labels)\n\nacc = accuracy(X_test, y_test)\nprint(f\"\\nTest Accuracy: {acc * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:05:56.954974Z","iopub.execute_input":"2025-04-12T14:05:56.955277Z","iopub.status.idle":"2025-04-12T14:05:57.132233Z","shell.execute_reply.started":"2025-04-12T14:05:56.955256Z","shell.execute_reply":"2025-04-12T14:05:57.131360Z"}},"outputs":[{"name":"stdout","text":"Epoch 0 - Loss: 2.5629\n\n--- Gradient Check on w1[0,0] ---\n--- Gradient Check on w1[0,0] ---\nAnalytical: 0.00000007 | Numerical: 0.00000000 | Diff: 0.00000007\nEpoch 50 - Loss: 0.0539\nEpoch 100 - Loss: 0.0494\nEpoch 150 - Loss: 0.0457\nEpoch 200 - Loss: 0.0323\nEpoch 250 - Loss: 0.0265\nEpoch 300 - Loss: 0.0220\nEpoch 350 - Loss: 0.0187\nEpoch 400 - Loss: 0.0156\nEpoch 450 - Loss: 0.0141\n\nTest Accuracy: 100.00%\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"def gradient_check(X, y, w1, b1, w2, b2, w3, b3, dw1, index):\n    epsilon = 1e-5\n    i, j = index\n    \n    original_val = w1[i, j]\n\n    w1[i, j] = original_val + epsilon\n    out1_plus = layer(X, w1, b1, 'relu')\n    out2_plus = layer(out1_plus, w2, b2, 'relu')\n    out3_plus = layer(out2_plus, w3, b3, 'softmax')\n    loss_plus = cross_entropy_loss(out3_plus, y)\n\n    w1[i, j] = original_val - epsilon\n    out1_minus = layer(X, w1, b1, 'relu')\n    out2_minus = layer(out1_minus, w2, b2, 'relu')\n    out3_minus = layer(out2_minus, w3, b3, 'softmax')\n    loss_minus = cross_entropy_loss(out3_minus, y)\n\n    # Reset w1\n    w1[i, j] = original_val\n\n    # Numerical Gradient\n    num_grad = (loss_plus - loss_minus) / (2 * epsilon)\n    ana_grad = dw1[i, j]\n\n    print(f\"--- Gradient Check on w1[{i},{j}] ---\")\n    print(f\"Analytical: {ana_grad:.8f} | Numerical: {num_grad:.8f} | Diff: {abs(ana_grad - num_grad):.8f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:05:58.107013Z","iopub.execute_input":"2025-04-12T14:05:58.107287Z","iopub.status.idle":"2025-04-12T14:05:58.114465Z","shell.execute_reply.started":"2025-04-12T14:05:58.107269Z","shell.execute_reply":"2025-04-12T14:05:58.113354Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"# Neural Network with L2 Regularization","metadata":{}},{"cell_type":"code","source":"def cross_entropy_loss_with_regularization(y_pred, y_true, model_weights, lambda_reg):\n    m = y_true.shape[0]\n    loss = -np.sum(y_true * np.log(y_pred + 1e-9)) / m\n    \n    # L2 Regularization Term\n    l2_reg = lambda_reg * np.sum(np.square(model_weights))  # Sum of squares of all weights\n    \n    total_loss = loss + l2_reg  # Total loss with regularization\n    return total_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:06:03.091539Z","iopub.execute_input":"2025-04-12T14:06:03.091936Z","iopub.status.idle":"2025-04-12T14:06:03.097481Z","shell.execute_reply.started":"2025-04-12T14:06:03.091907Z","shell.execute_reply":"2025-04-12T14:06:03.096601Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"input_size = X_train.shape[1]  # 4\nhidden_size = 10\noutput_size = 3  # 3 classes in Iris\n\n# Weights & Biases\nw1 = np.random.randn(input_size, hidden_size)\nb1 = np.zeros((1, hidden_size))\nw2 = np.random.randn(hidden_size, hidden_size)\nb2 = np.zeros((1, hidden_size))\nw3 = np.random.randn(hidden_size, output_size)\nb3 = np.zeros((1, output_size))\n\n# Hyperparameter for regularization\nlambda_reg = 0.3# You can tune this value\nepochs = 500\n\n# Training loop\nfor epoch in range(epochs):\n    # Forward Pass\n    z1 = layer(X_train, w1, b1, activation='None')\n    out1 = layer(X_train, w1, b1, activation=\"relu\")\n\n    z2 = layer(out1, w2, b2, activation='None')\n    out2 = layer(out1, w2, b2, activation=\"relu\")\n\n    z3 = layer(out2, w3, b3, activation='None')\n    out3 = layer(out2, w3, b3, activation='softmax')\n\n    # Loss Calculation with Regularization\n    loss = cross_entropy_loss_with_regularization(out3, y_train, np.concatenate([w1.flatten(), w2.flatten(), w3.flatten()]), lambda_reg)\n\n    # Backward Pass (with L2 gradient update)\n    dz3 = softmax_derivative(z3, y_train)\n    dw3 = np.dot(out2.T, dz3)\n    db3 = np.sum(dz3, axis=0, keepdims=True)\n\n    dz2 = np.dot(dz3, w3.T) * relu_derivative(z2)\n    dw2 = np.dot(out1.T, dz2)\n    db2 = np.sum(dz2, axis=0, keepdims=True)\n\n    dz1 = np.dot(dz2, w2.T) * relu_derivative(z1)\n    dw1 = np.dot(X_train.T, dz1)\n    db1 = np.sum(dz1, axis=0, keepdims=True)\n\n    # L2 Regularization on Gradients\n    dw1 += 2 * lambda_reg * w1\n    dw2 += 2 * lambda_reg * w2\n    dw3 += 2 * lambda_reg * w3\n\n    # Update weights\n    w3 -= lr * dw3\n    b3 -= lr * db3\n    w2 -= lr * dw2\n    b2 -= lr * db2\n    w1 -= lr * dw1\n    b1 -= lr * db1\n\n    if epoch % 50 == 0:\n        print(f\"Epoch {epoch} - Loss: {loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:08:19.629638Z","iopub.execute_input":"2025-04-12T14:08:19.629918Z","iopub.status.idle":"2025-04-12T14:08:19.793375Z","shell.execute_reply.started":"2025-04-12T14:08:19.629899Z","shell.execute_reply":"2025-04-12T14:08:19.792561Z"}},"outputs":[{"name":"stdout","text":"Epoch 0 - Loss: 46.9595\nEpoch 50 - Loss: 38.7112\nEpoch 100 - Loss: 24.1473\nEpoch 150 - Loss: 15.9221\nEpoch 200 - Loss: 11.9210\nEpoch 250 - Loss: 9.8292\nEpoch 300 - Loss: 8.6446\nEpoch 350 - Loss: 7.8743\nEpoch 400 - Loss: 7.3548\nEpoch 450 - Loss: 7.0950\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"acc = accuracy(X_test, y_test)\nprint(f\"\\nTest Accuracy: {acc * 100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:08:20.827725Z","iopub.execute_input":"2025-04-12T14:08:20.828013Z","iopub.status.idle":"2025-04-12T14:08:20.834097Z","shell.execute_reply.started":"2025-04-12T14:08:20.827992Z","shell.execute_reply":"2025-04-12T14:08:20.833110Z"}},"outputs":[{"name":"stdout","text":"\nTest Accuracy: 96.67%\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}